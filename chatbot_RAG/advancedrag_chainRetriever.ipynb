{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data ingestion - reading from pdf, documents, etc\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"attention.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions', metadata={'source': 'attention.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform - made chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents=splitter.split_documents(docs)\n",
    "documents[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db=Chroma.from_documents(documents, OllamaEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Please provide excellent answer for the user query.\n",
    "                                          <context>{context}</context>\n",
    "                                          question:{input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain llm and prompt\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# retriever with vector db\n",
    "ret = db.as_retriever()\n",
    "\n",
    "# combine chain, retriever\n",
    "from langchain.chains import create_retrieval_chain\n",
    "chain_ret = create_retrieval_chain(ret,chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The question you provided is related to the comparison of self-attention layers and recurrent layers in neural machine translation models. Specifically, you are interested in understanding how these layers differ in terms of computational complexity and executed operations.\\n\\nTo answer your question, I will provide a brief overview of self-attention layers and recurrent layers, followed by a comparison of their computational complexity and executed operations.\\n\\nSelf-Attention Layers:\\nSelf-attention layers are a type of neural network layer that allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. In a self-attention layer, the output is computed as a weighted sum of the input values, where the weights are learned during training. Self-attention layers are particularly useful in natural language processing tasks, such as machine translation, as they can capture long-range dependencies in the input sequence.\\n\\nRecurrent Layers:\\nRecurrent layers, on the other hand, are a type of neural network layer that processes the input sequence sequentially. In a recurrent layer, the output at each time step is determined by the current input and the previous hidden state. Recurrent layers are useful in modeling temporal dependencies in sequences, such as language models or speech recognition systems.\\n\\nComparison of Computational Complexity and Executed Operations:\\nIn terms of computational complexity, self-attention layers are faster than recurrent layers when dealing with long sequences. This is because self-attention layers only require a constant number of operations per input element, whereas recurrent layers require sequential operations that scale with the length of the input sequence. Specifically, the computational complexity of self-attention layers is O(1), while that of recurrent layers is O(n), where n is the length of the input sequence.\\n\\nIn terms of executed operations, self-attention layers typically require more operations than recurrent layers, especially when dealing with long sequences. This is because self-attention layers need to compute the attention weights for each element in the input sequence, which can lead to a higher number of operations. However, this trade-off can be beneficial in certain tasks, such as machine translation, where the model needs to capture long-range dependencies in the input sequence.\\n\\nIn summary, self-attention layers and recurrent layers differ in terms of their computational complexity and executed operations. While self-attention layers are faster and more efficient for processing long sequences, they may require more operations than recurrent layers to capture complex contextual relationships in the input sequence. The choice between these layers depends on the specific task requirements and the available computational resources.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chain_ret.invoke({'input':\"WMT 2014 English-German dataset\"})\n",
    "res['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Python is a high-level programming language that is widely used for various applications such as web development, data analysis, machine learning, and more. Here are some key features of Python:\\n\\n1. Interpreted Language: Python is an interpreted language, meaning that it can execute code directly without the need for compiling it first. This makes it a versatile language that can be used for rapid prototyping and development.\\n2. Simple Syntax: Python has a simple syntax with clear guidelines for indentation, spacing, and commenting code. This makes it easier to read and write compared to other programming languages.\\n3. Large Standard Library: Python has a vast standard library that includes modules for various tasks such as file I/O, networking, and data structures. It also has an extensive collection of third-party libraries and frameworks that can be used to perform specific tasks.\\n4. Object-Oriented Programming: Python supports object-oriented programming (OOP) concepts such as classes, objects, inheritance, and polymorphism. This makes it easy to write reusable code and create complex programs with modular structures.\\n5. Dynamic Typing: Python is dynamically typed, meaning that you don't need to declare the data type of variables before using them. This makes it easier to write code quickly and experiment with different ideas without worrying about syntax errors.\\n6. Extensive Community Support: Python has a large and active community of developers who contribute to its growth and development. There are numerous online resources, forums, and libraries available for Python users.\\n7. Cross-Platform Compatibility: Python can be run on various operating systems such as Windows, macOS, Linux, and more. It also has implementations for different hardware platforms such as Raspberry Pi and Arduino.\\n8. Extensive Use in Industries: Python is widely used in industries such as web development, data science, machine learning, artificial intelligence, and more. Many popular websites and applications use Python as their back-end language.\\n9. Libraries for Data Science: Python has several libraries for data science tasks such as NumPy, pandas, and scikit-learn. These libraries provide efficient algorithms for data manipulation and analysis, making it easier to work with large datasets.\\n10. Large Application Ecosystem: Python has a vast ecosystem of applications that can be used for various tasks such as web development, scientific computing, data visualization, and more. Popular applications include Django, Flask, Pyramid, and Matplotlib.\\n\\nIn summary, Python is a versatile language with a simple syntax, extensive standard library, and large community support. Its dynamic typing, cross-platform compatibility, and extensive use in industries make it a popular choice for rapid prototyping and development.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chain_ret.invoke({'input':\"what is python?\"})\n",
    "res['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
